\documentclass[12]{extarticle}

\usepackage{amsmath}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{algpseudocode}

\begin{document}

\title{Introduction to Neuro-Evolution: Solutions}
\author{Andrew Geng, Jonas Klare, John Balis}




\maketitle





\section{Simulated Annealing}

\subsection{}

\subsection{}

\subsection{}

\section{Mutation and Speciation}


\section{Neuro-Evolution}
\subsection{A}
Possible observations: 

Either none of the training runs will result in significant survival time or one or more training runs will result in signficant survival time, and will be seen to exhibit  some sort of behavior with the intent of avoiding the hunter. 

It's important to note that neuro-evolution is very unpredictable, and may fail to find a good solution even after a very long time. However, running a given training run for longer can only improve the performance of the network.


\subsection{B}
 You should observe that the sample network has a very effective a deliberate style of avoiding the hunter. All successful SSNs we have trained have employed a variant of this style. It's important to note that while is no upper bound on how long it takes to train a network to this quality and we could not leave it's generation as an exercise, it is possible to train a network to this quality in even 1000 or fewer epochs. 


\subsection{C}

Possible Obervations:

Either increasing the speed will result in no significant improvement over the baseline(if the baseline fails to learn the task effectively) or will result in a dramatic decrease from the baseline.

The premade network will still perform reasonably.

This illustrates that when designing a training instance, it is generally desirable to reduce the difficulty of the task for early training, and then gradually increase difficulty as score increases.






\end{document}

